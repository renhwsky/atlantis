# 基本概念

## RDD

RDD 全称为 Resilient Distributed Datasets，即弹性分布式数据集.


## Shuffle

只有当 Shuffle 依赖中父 RDD 所有分区的数据被计算和存储完毕之后，子 RDD 才会开始去拉取需要的分区数据。我们将整个数据传输的过程称为 Apache Spark 的 `Shuffle过程`。Shuffle 过程中，我们把一个分区数据计算完毕到数据被写入到磁盘的过程，称为 `Shuffle写过程`。对应的，在子 RDD 某个分区计算的过程中，把所需的数据从父 RDD 拉取过来的过程，称为 `Shuffle读过程`。

`Shuffle`的实现参考[Shuffle 过程](https://github.com/ihainan/SparkInternals/tree/master/section3).

## Task scheduler

一个Spark应用程序包括Job、Stage以及Task三个概念：

- Job是以Action方法为界，遇到一个Action方法则触发一个Job；
- Stage是Job的子集，以RDD宽依赖(即Shuffle)为界，遇到Shuffle做一次划分；
- Task是Stage的子集，以并行度(分区数)来衡量，分区数是多少，则有多少个task。

Spark的任务调度总体来说分两级进行，一是Stage级的调度，一是Task级的调度。


## 资源分配
